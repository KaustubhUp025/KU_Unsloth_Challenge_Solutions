{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOm4N7quU1wQ3MVjfroO7yt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1da9fa52b57348d1bd2cea12137603ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3dfa3ad962f14793afeddfb8e24d82a8",
              "IPY_MODEL_1d964439a4ce468aad4e1e7135ed946b",
              "IPY_MODEL_c014642b2809475eabaa8962bcb7960d"
            ],
            "layout": "IPY_MODEL_6db13341db694b85adf8c39094fa1d23"
          }
        },
        "3dfa3ad962f14793afeddfb8e24d82a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0df6a6d97a7a4352a0a6b3fd4905a8cd",
            "placeholder": "​",
            "style": "IPY_MODEL_40bd31d8e63c4ed99d7903b8535d6d94",
            "value": "Tokenizing train dataset (num_proc=4): 100%"
          }
        },
        "1d964439a4ce468aad4e1e7135ed946b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_748a711593e84a55a347ee536c7ad840",
            "max": 21029,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c12cd02faad74dd88744e08a89193bff",
            "value": 21029
          }
        },
        "c014642b2809475eabaa8962bcb7960d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2c1e6f637b44b55811d1b25932cb366",
            "placeholder": "​",
            "style": "IPY_MODEL_336ab9a61b7143dc92ea844133bcf420",
            "value": " 21029/21029 [00:18&lt;00:00, 1276.77 examples/s]"
          }
        },
        "6db13341db694b85adf8c39094fa1d23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0df6a6d97a7a4352a0a6b3fd4905a8cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40bd31d8e63c4ed99d7903b8535d6d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "748a711593e84a55a347ee536c7ad840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c12cd02faad74dd88744e08a89193bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2c1e6f637b44b55811d1b25932cb366": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "336ab9a61b7143dc92ea844133bcf420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2232c462f36446f48bf1601a7ed9e994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f44643ab94e4c2783dae512a7cd4100",
              "IPY_MODEL_4181d31ad2bd42839ba321ab23809d7a",
              "IPY_MODEL_9982d6b419624a95953d2cd40d8a973a"
            ],
            "layout": "IPY_MODEL_227bdf3eaa9644cdb899feab825e164c"
          }
        },
        "5f44643ab94e4c2783dae512a7cd4100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d2fe1e3f19541b8ba164cad156c6715",
            "placeholder": "​",
            "style": "IPY_MODEL_61792895e7f349a48971f45e48a7e214",
            "value": "Tokenizing train dataset (num_proc=4): 100%"
          }
        },
        "4181d31ad2bd42839ba321ab23809d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad7a53cbc17b44d58c832ecf987f73cb",
            "max": 21029,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_142e0167cd3c452796a8e60f33aae64d",
            "value": 21029
          }
        },
        "9982d6b419624a95953d2cd40d8a973a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f30336a37ce649c9a0f5bdf1c32986b3",
            "placeholder": "​",
            "style": "IPY_MODEL_27d35cc220b64257a908a93ee9dc0515",
            "value": " 21029/21029 [00:07&lt;00:00, 3738.10 examples/s]"
          }
        },
        "227bdf3eaa9644cdb899feab825e164c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d2fe1e3f19541b8ba164cad156c6715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61792895e7f349a48971f45e48a7e214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad7a53cbc17b44d58c832ecf987f73cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "142e0167cd3c452796a8e60f33aae64d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f30336a37ce649c9a0f5bdf1c32986b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27d35cc220b64257a908a93ee9dc0515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaustubhUp025/KU_Unsloth_Challenge_Solutions/blob/main/ChallengeCSolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCGfABTYU2n2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shLagaUjU9e6",
        "outputId": "e621ea4d-f3ed-440e-bc58-484f9b3dfc58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers.models.llama.modeling_llama as llama_mod\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import torch._dynamo\n",
        "import torch._inductor\n",
        "import logging\n",
        "import torch.autograd as autograd\n",
        "import bitsandbytes.functional as F  # BitsAndBytes functional API\n",
        "from types import SimpleNamespace  # For quant_state attributes\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Patch the transpose method of BitsAndBytes Params4bit so that it runs in eager mode.\n",
        "# This prevents TorchDynamo from tracing user-defined methods on BitsAndBytes objects.\n",
        "# -----------------------------------------------------------------------------\n",
        "import bitsandbytes as bnb\n",
        "if hasattr(bnb, \"Params4bit\"):\n",
        "    _orig_t = bnb.Params4bit.t\n",
        "    @torch._dynamo.disable\n",
        "    def safe_t(self):\n",
        "        return _orig_t(self)\n",
        "    bnb.Params4bit.t = safe_t\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Eager dequantization+transpose wrapper for BitsAndBytes that is not traced.\n",
        "# -----------------------------------------------------------------------------\n",
        "@torch._dynamo.disable\n",
        "def dequantize_4bit_eager(weight, quant_state, dtype):\n",
        "    result = F.dequantize_4bit(weight, quant_state).to(dtype).t()\n",
        "    return result\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Custom 4-bit MatMul Operator using the original dequantization behavior.\n",
        "# -----------------------------------------------------------------------------\n",
        "class CustomMatMul4BitFunction(autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, weight, bias, quant_state):\n",
        "        deq_weight = dequantize_4bit_eager(weight, quant_state, x.dtype)\n",
        "        ctx.save_for_backward(x, deq_weight, bias)\n",
        "        output = torch.nn.functional.linear(x, deq_weight, bias)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, deq_weight, bias = ctx.saved_tensors\n",
        "        x_flat = x.reshape(-1, x.shape[-1]).to(grad_output.dtype)\n",
        "        grad_output_flat = grad_output.reshape(-1, grad_output.shape[-1])\n",
        "        deq_weight = deq_weight.to(grad_output.dtype)\n",
        "        grad_x_flat = grad_output_flat.matmul(deq_weight)\n",
        "        grad_weight = grad_output_flat.transpose(0, 1).matmul(x_flat)\n",
        "        grad_x = grad_x_flat.reshape(x.shape)\n",
        "        grad_bias = grad_output_flat.sum(dim=0) if bias is not None else None\n",
        "        return grad_x, grad_weight, grad_bias, None\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Wrapper for 4-bit matmul.\n",
        "# This function ensures quant_state is a Tensor carrying the necessary attributes.\n",
        "# We disable its tracing so that dynamic operations (like setattr) are not traced.\n",
        "# -----------------------------------------------------------------------------\n",
        "@torch._dynamo.disable\n",
        "def matmul_4bit_wrapper(x, weight, bias, quant_state):\n",
        "    if not isinstance(quant_state, torch.Tensor):\n",
        "        dummy_shape = getattr(quant_state, \"shape\", weight.shape)\n",
        "        dummy = torch.empty(dummy_shape, device=x.device, dtype=x.dtype)\n",
        "        for attr in [\"absmax\", \"nested\", \"state2\", \"blocksize\", \"quant_type\", \"offset\"]:\n",
        "            if hasattr(quant_state, attr):\n",
        "                setattr(dummy, attr, getattr(quant_state, attr))\n",
        "        quant_state = dummy\n",
        "    return CustomMatMul4BitFunction.apply(x, weight, bias, quant_state)\n",
        "\n",
        "_original_matmul_4bit = bnb.matmul_4bit  # Save original if needed.\n",
        "bnb.matmul_4bit = matmul_4bit_wrapper\n",
        "\n",
        "######################################\n",
        "# Torch compile options and Model Patching (unchanged from original)\n",
        "######################################\n",
        "torch_compile_options = {\n",
        "    \"epilogue_fusion\": True,\n",
        "    \"max_autotune\": True,\n",
        "    \"shape_padding\": True,\n",
        "    \"trace.enabled\": True,\n",
        "    \"triton.cudagraphs\": False,\n",
        "}\n",
        "\n",
        "try:\n",
        "    torch.compiler.allow_in_graph(bnb.nn.modules.Params4bit.t)\n",
        "except Exception as e:\n",
        "    print(\"Warning: Could not mark Params4bit.t as allowed:\", e)\n",
        "\n",
        "@torch.compile(fullgraph=False, dynamic=True, options=torch_compile_options)\n",
        "def compiled_llama_mlp(self, x):\n",
        "    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "llama_mod.LlamaMLP.forward = compiled_llama_mlp\n",
        "\n",
        "@torch.compile(fullgraph=False, dynamic=True, options=torch_compile_options)\n",
        "def compiled_flex_attention(self, hidden_states, position_embeddings, attention_mask, past_key_value=None, cache_position=None, **kwargs):\n",
        "    # --- New patch: disable caching during training to avoid dynamic past_key_value issues ---\n",
        "    if self.training:\n",
        "        past_key_value = None\n",
        "\n",
        "    batch_size, seq_len, _ = hidden_states.shape\n",
        "    num_query_heads = self.q_proj.out_features // self.head_dim\n",
        "    num_kv_heads    = self.k_proj.out_features // self.head_dim\n",
        "\n",
        "    query_states = self.q_proj(hidden_states).reshape(batch_size, seq_len, num_query_heads, self.head_dim).transpose(1, 2)\n",
        "    key_states   = self.k_proj(hidden_states).reshape(batch_size, seq_len, num_kv_heads, self.head_dim).transpose(1, 2)\n",
        "    value_states = self.v_proj(hidden_states).reshape(batch_size, seq_len, num_kv_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    cos, sin = position_embeddings\n",
        "    cos = cos[:seq_len, :] if cos.dim() == 2 else cos[..., :seq_len, :]\n",
        "    sin = sin[:seq_len, :] if sin.dim() == 2 else sin[..., :seq_len, :]\n",
        "\n",
        "    query_states, key_states = llama_mod.apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "    if past_key_value is not None:\n",
        "        cache_kwargs = {\"sin\": cos, \"cos\": sin, \"cache_position\": cache_position}\n",
        "        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
        "\n",
        "    from transformers.models.llama.modeling_llama import repeat_kv\n",
        "    key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "    value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
        "        query_states, key_states, value_states,\n",
        "        attn_mask=attention_mask,\n",
        "        dropout_p=self.attention_dropout if self.training else 0.0,\n",
        "        is_causal=self.is_causal\n",
        "    )\n",
        "\n",
        "    attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, num_query_heads * self.head_dim)\n",
        "    attn_output = self.o_proj(attn_output)\n",
        "    return attn_output, None\n",
        "\n",
        "llama_mod.LlamaAttention.forward = compiled_flex_attention\n",
        "\n",
        "original_layernorm_forward = nn.LayerNorm.forward\n",
        "@torch.compile(fullgraph=True, dynamic=True, options=torch_compile_options)\n",
        "def compiled_layernorm_forward(self, input):\n",
        "    return original_layernorm_forward(self, input)\n",
        "nn.LayerNorm.forward = compiled_layernorm_forward\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "max_seq_length = 1024\n",
        "torch.set_default_dtype(torch.float16)\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "dtype = torch.float16\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=dtype,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"sdpa\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# ----- Patch quant_state for quantized weights only if needed ---------\n",
        "with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "        if hasattr(param, \"quant_state\") and hasattr(param.quant_state, \"shape\"):\n",
        "            if param.quant_state.shape[0] == 1:\n",
        "                if hasattr(param.quant_state, \"logical_shape\"):\n",
        "                    new_shape = param.quant_state.logical_shape\n",
        "                else:\n",
        "                    new_shape = param.quant_state.shape  # fallback; leave as is\n",
        "                if new_shape != param.quant_state.shape:\n",
        "                    print(f\"Patching quant_state shape for {name}: {param.quant_state.shape} -> {new_shape}\")\n",
        "                    param.quant_state.shape = new_shape\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "        if \".lora_A.\" in name or \".lora_B.\" in name:\n",
        "            param.requires_grad_(True)\n",
        "        else:\n",
        "            param.requires_grad_(False)\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": url}, split=\"train[:10%]\")\n",
        "\n",
        "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
        "\n",
        "torch._dynamo.config.force_parameter_static_shapes = False\n",
        "torch._dynamo.config.capture_dynamic_output_shape_ops = True\n",
        "\n",
        "torch._inductor.config.debug = True\n",
        "torch._logging.set_logs(\n",
        "    dynamo=logging.WARN,\n",
        "    inductor=logging.WARN,\n",
        "    graph_breaks=True,\n",
        "    recompiles=True,\n",
        "    recompiles_verbose=True,\n",
        "    compiled_autograd_verbose=True,\n",
        ")\n",
        "torch._dynamo.config.verbose = True\n",
        "torch._dynamo.config.suppress_errors = False\n",
        "\n",
        "def log_gpu_stats():\n",
        "    allocated = torch.cuda.memory_allocated()\n",
        "    cached = torch.cuda.memory_reserved()\n",
        "    print(f\"Allocated VRAM: {allocated / 1e6:.1f} MB, Reserved: {cached / 1e6:.1f} MB\")\n",
        "log_gpu_stats()\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=1,\n",
        "        max_steps=10,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        seed=3407,\n",
        "        max_seq_length=max_seq_length,\n",
        "        fp16=(model.get_input_embeddings().weight.dtype == torch.float16),\n",
        "        bf16=(model.get_input_embeddings().weight.dtype == torch.bfloat16),\n",
        "        report_to=\"none\",\n",
        "        dataset_num_proc=4,\n",
        "    ),\n",
        ")\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1da9fa52b57348d1bd2cea12137603ef",
            "3dfa3ad962f14793afeddfb8e24d82a8",
            "1d964439a4ce468aad4e1e7135ed946b",
            "c014642b2809475eabaa8962bcb7960d",
            "6db13341db694b85adf8c39094fa1d23",
            "0df6a6d97a7a4352a0a6b3fd4905a8cd",
            "40bd31d8e63c4ed99d7903b8535d6d94",
            "748a711593e84a55a347ee536c7ad840",
            "c12cd02faad74dd88744e08a89193bff",
            "c2c1e6f637b44b55811d1b25932cb366",
            "336ab9a61b7143dc92ea844133bcf420",
            "2232c462f36446f48bf1601a7ed9e994",
            "5f44643ab94e4c2783dae512a7cd4100",
            "4181d31ad2bd42839ba321ab23809d7a",
            "9982d6b419624a95953d2cd40d8a973a",
            "227bdf3eaa9644cdb899feab825e164c",
            "1d2fe1e3f19541b8ba164cad156c6715",
            "61792895e7f349a48971f45e48a7e214",
            "ad7a53cbc17b44d58c832ecf987f73cb",
            "142e0167cd3c452796a8e60f33aae64d",
            "f30336a37ce649c9a0f5bdf1c32986b3",
            "27d35cc220b64257a908a93ee9dc0515"
          ]
        },
        "id": "S95MCzQpU_nZ",
        "outputId": "ad471104-c8c5-43df-9ad4-1841f18d93a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:195: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated VRAM: 1118.9 MB, Reserved: 1153.4 MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset (num_proc=4):   0%|          | 0/21029 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1da9fa52b57348d1bd2cea12137603ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset (num_proc=4):   0%|          | 0/21029 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2232c462f36446f48bf1601a7ed9e994"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "V0221 12:19:38.816000 9693 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0221 12:19:38.816000 9693 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0221 12:19:38.816000 9693 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] User code traceback:\n",
            "V0221 12:19:38.816000 9693 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File \"<ipython-input-3-9e33b440181a>\", line 108, in compiled_flex_attention\n",
            "V0221 12:19:38.816000 9693 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     query_states = self.q_proj(hidden_states).reshape(batch_size, seq_len, num_query_heads, self.head_dim).transpose(1, 2)\n",
            "V0221 12:19:38.816000 9693 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0221 12:19:38.816000 9693 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0221 12:19:38.816000 9693 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0221 12:19:38.816000 9693 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0221 12:19:38.816000 9693 torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] \n",
            "W0221 12:19:42.673000 9693 torch/_inductor/debug.py:435] [0/0_1] model__0_inference_0 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__0_inference_0.0\n",
            "V0221 12:19:42.791000 9693 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0221 12:19:42.791000 9693 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0221 12:19:42.791000 9693 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks] User code traceback:\n",
            "V0221 12:19:42.791000 9693 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0221 12:19:42.791000 9693 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0221 12:19:42.791000 9693 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0221 12:19:42.791000 9693 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0221 12:19:42.791000 9693 torch/_dynamo/symbolic_convert.py:435] [1/0] [__graph_breaks] \n",
            "V0221 12:19:42.914000 9693 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0221 12:19:42.914000 9693 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0221 12:19:42.914000 9693 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] User code traceback:\n",
            "V0221 12:19:42.914000 9693 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0221 12:19:42.914000 9693 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0221 12:19:42.914000 9693 torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] \n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "W0221 12:19:43.845000 9693 torch/_inductor/debug.py:435] [2/0_1] model__1_forward_2 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__1_forward_2.1\n",
            "W0221 12:19:46.124000 9693 torch/_inductor/debug.py:435] [5/0] model__2_forward_4 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__2_forward_4.2\n",
            "W0221 12:19:46.207000 9693 torch/_inductor/debug.py:435] [5/0] model__2_backward_5 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__2_backward_5.3\n",
            "V0221 12:19:46.510000 9693 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0221 12:19:46.510000 9693 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0221 12:19:46.510000 9693 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks] User code traceback:\n",
            "V0221 12:19:46.510000 9693 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]   File \"<ipython-input-3-9e33b440181a>\", line 109, in torch_dynamo_resume_in_compiled_flex_attention_at_108\n",
            "V0221 12:19:46.510000 9693 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]     key_states   = self.k_proj(hidden_states).reshape(batch_size, seq_len, num_kv_heads, self.head_dim).transpose(1, 2)\n",
            "V0221 12:19:46.510000 9693 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0221 12:19:46.510000 9693 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0221 12:19:46.510000 9693 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0221 12:19:46.510000 9693 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0221 12:19:46.510000 9693 torch/_dynamo/symbolic_convert.py:435] [6/0] [__graph_breaks] \n",
            "W0221 12:19:46.808000 9693 torch/_inductor/debug.py:435] [6/0_1] model__3_inference_6 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__3_inference_6.4\n",
            "V0221 12:19:46.963000 9693 torch/_dynamo/guards.py:2789] [5/1] [__recompiles_verbose] Recompiling function torch_dynamo_resume_in_forward_at_496 in /usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:496\n",
            "V0221 12:19:46.963000 9693 torch/_dynamo/guards.py:2789] [5/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0221 12:19:46.963000 9693 torch/_dynamo/guards.py:2789] [5/1] [__recompiles_verbose]     guard 0 failures:\n",
            "V0221 12:19:46.963000 9693 torch/_dynamo/guards.py:2789] [5/1] [__recompiles_verbose]     - 5/0: tensor 'L['self']._modules['lora_B']._modules['default']._parameters['weight']' size mismatch at index 0. expected 2048, actual 512. Guard failed on a parameter, consider using torch._dynamo.config.force_parameter_static_shapes = False to allow dynamism on parameters.\n",
            "W0221 12:19:47.503000 9693 torch/_inductor/debug.py:435] [5/1] model__4_forward_8 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__4_forward_8.5\n",
            "W0221 12:19:47.584000 9693 torch/_inductor/debug.py:435] [5/1] model__4_backward_9 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__4_backward_9.6\n",
            "V0221 12:19:47.743000 9693 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0221 12:19:47.743000 9693 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0221 12:19:47.743000 9693 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks] User code traceback:\n",
            "V0221 12:19:47.743000 9693 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]   File \"<ipython-input-3-9e33b440181a>\", line 110, in torch_dynamo_resume_in_compiled_flex_attention_at_109\n",
            "V0221 12:19:47.743000 9693 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]     value_states = self.v_proj(hidden_states).reshape(batch_size, seq_len, num_kv_heads, self.head_dim).transpose(1, 2)\n",
            "V0221 12:19:47.743000 9693 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0221 12:19:47.743000 9693 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0221 12:19:47.743000 9693 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0221 12:19:47.743000 9693 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0221 12:19:47.743000 9693 torch/_dynamo/symbolic_convert.py:435] [7/0] [__graph_breaks] \n",
            "W0221 12:19:47.921000 9693 torch/_inductor/debug.py:435] [7/0_1] model__5_inference_10 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__5_inference_10.7\n",
            "V0221 12:19:49.139000 9693 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0221 12:19:49.139000 9693 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0221 12:19:49.139000 9693 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks] User code traceback:\n",
            "V0221 12:19:49.139000 9693 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks]   File \"<ipython-input-3-9e33b440181a>\", line 134, in torch_dynamo_resume_in_compiled_flex_attention_at_110\n",
            "V0221 12:19:49.139000 9693 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks]     attn_output = self.o_proj(attn_output)\n",
            "V0221 12:19:49.139000 9693 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0221 12:19:49.139000 9693 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0221 12:19:49.139000 9693 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0221 12:19:49.139000 9693 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0221 12:19:49.139000 9693 torch/_dynamo/symbolic_convert.py:435] [8/0] [__graph_breaks] \n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0221 12:19:53.890000 9693 torch/_inductor/debug.py:435] [8/0_1] model__6_forward_12 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__6_forward_12.8\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0221 12:19:55.720000 9693 torch/_inductor/debug.py:435] [8/0_1] model__6_backward_13 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__6_backward_13.9\n",
            "V0221 12:19:57.000000 9693 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0221 12:19:57.000000 9693 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0221 12:19:57.000000 9693 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks] User code traceback:\n",
            "V0221 12:19:57.000000 9693 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks]   File \"<ipython-input-3-9e33b440181a>\", line 95, in compiled_llama_mlp\n",
            "V0221 12:19:57.000000 9693 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks]     return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0221 12:19:57.000000 9693 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0221 12:19:57.000000 9693 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0221 12:19:57.000000 9693 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0221 12:19:57.000000 9693 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0221 12:19:57.000000 9693 torch/_dynamo/symbolic_convert.py:435] [10/0] [__graph_breaks] \n",
            "V0221 12:19:57.039000 9693 torch/_dynamo/guards.py:2789] [5/2] [__recompiles_verbose] Recompiling function torch_dynamo_resume_in_forward_at_496 in /usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:496\n",
            "V0221 12:19:57.039000 9693 torch/_dynamo/guards.py:2789] [5/2] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0221 12:19:57.039000 9693 torch/_dynamo/guards.py:2789] [5/2] [__recompiles_verbose]     guard 0 failures:\n",
            "V0221 12:19:57.039000 9693 torch/_dynamo/guards.py:2789] [5/2] [__recompiles_verbose]     - 5/0: tensor 'L['self']._modules['lora_B']._modules['default']._parameters['weight']' size mismatch at index 0. expected 2048, actual 8192. Guard failed on a parameter, consider using torch._dynamo.config.force_parameter_static_shapes = False to allow dynamism on parameters.\n",
            "V0221 12:19:57.039000 9693 torch/_dynamo/guards.py:2789] [5/2] [__recompiles_verbose] \n",
            "V0221 12:19:57.039000 9693 torch/_dynamo/guards.py:2789] [5/2] [__recompiles_verbose]     guard 1 failures:\n",
            "V0221 12:19:57.039000 9693 torch/_dynamo/guards.py:2789] [5/2] [__recompiles_verbose]     - 5/1: tensor 'L['self']._modules['lora_B']._modules['default']._parameters['weight']' size mismatch at index 0. expected 512, actual 8192. Guard failed on a parameter, consider using torch._dynamo.config.force_parameter_static_shapes = False to allow dynamism on parameters.\n",
            "W0221 12:19:57.589000 9693 torch/_inductor/debug.py:435] [5/2] model__7_forward_15 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__7_forward_15.10\n",
            "W0221 12:19:57.656000 9693 torch/_inductor/debug.py:435] [5/2] model__7_backward_16 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__7_backward_16.11\n",
            "V0221 12:19:57.872000 9693 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0221 12:19:57.872000 9693 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0221 12:19:57.872000 9693 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks] User code traceback:\n",
            "V0221 12:19:57.872000 9693 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks]   File \"<ipython-input-3-9e33b440181a>\", line 95, in torch_dynamo_resume_in_compiled_llama_mlp_at_95\n",
            "V0221 12:19:57.872000 9693 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks]     return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0221 12:19:57.872000 9693 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0221 12:19:57.872000 9693 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0221 12:19:57.872000 9693 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0221 12:19:57.872000 9693 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0221 12:19:57.872000 9693 torch/_dynamo/symbolic_convert.py:435] [11/0] [__graph_breaks] \n",
            "W0221 12:19:58.155000 9693 torch/_inductor/debug.py:435] [11/0_1] model__8_forward_18 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__8_forward_18.12\n",
            "W0221 12:19:58.184000 9693 torch/_inductor/debug.py:435] [11/0_1] model__8_backward_19 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__8_backward_19.13\n",
            "V0221 12:19:58.294000 9693 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks] Graph break in user code at /usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:484\n",
            "V0221 12:19:58.294000 9693 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks] Reason: Unsupported: call_method UserDefinedObjectVariable(Params4bit) t [] {}\n",
            "V0221 12:19:58.294000 9693 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks] User code traceback:\n",
            "V0221 12:19:58.294000 9693 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks]   File \"<ipython-input-3-9e33b440181a>\", line 95, in torch_dynamo_resume_in_compiled_llama_mlp_at_95\n",
            "V0221 12:19:58.294000 9693 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks]     return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "V0221 12:19:58.294000 9693 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n",
            "V0221 12:19:58.294000 9693 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks]     result = self.base_layer(x, *args, **kwargs)\n",
            "V0221 12:19:58.294000 9693 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks]   File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n",
            "V0221 12:19:58.294000 9693 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks]     return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n",
            "V0221 12:19:58.294000 9693 torch/_dynamo/symbolic_convert.py:435] [12/0] [__graph_breaks] \n",
            "W0221 12:19:58.417000 9693 torch/_inductor/debug.py:435] [12/0_1] model__9_forward_21 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__9_forward_21.14\n",
            "W0221 12:19:58.443000 9693 torch/_inductor/debug.py:435] [12/0_1] model__9_backward_22 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__9_backward_22.15\n",
            "V0221 12:19:58.466000 9693 torch/_dynamo/guards.py:2789] [5/3] [__recompiles_verbose] Recompiling function torch_dynamo_resume_in_forward_at_496 in /usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:496\n",
            "V0221 12:19:58.466000 9693 torch/_dynamo/guards.py:2789] [5/3] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
            "V0221 12:19:58.466000 9693 torch/_dynamo/guards.py:2789] [5/3] [__recompiles_verbose]     guard 0 failures:\n",
            "V0221 12:19:58.466000 9693 torch/_dynamo/guards.py:2789] [5/3] [__recompiles_verbose]     - 5/2: tensor 'L['x']' size mismatch at index 2. expected 2048, actual 8192\n",
            "V0221 12:19:58.466000 9693 torch/_dynamo/guards.py:2789] [5/3] [__recompiles_verbose] \n",
            "V0221 12:19:58.466000 9693 torch/_dynamo/guards.py:2789] [5/3] [__recompiles_verbose]     guard 1 failures:\n",
            "V0221 12:19:58.466000 9693 torch/_dynamo/guards.py:2789] [5/3] [__recompiles_verbose]     - 5/0: tensor 'L['self']._modules['lora_A']._modules['default']._parameters['weight']' size mismatch at index 1. expected 2048, actual 8192. Guard failed on a parameter, consider using torch._dynamo.config.force_parameter_static_shapes = False to allow dynamism on parameters.\n",
            "V0221 12:19:58.466000 9693 torch/_dynamo/guards.py:2789] [5/3] [__recompiles_verbose] \n",
            "V0221 12:19:58.466000 9693 torch/_dynamo/guards.py:2789] [5/3] [__recompiles_verbose]     guard 2 failures:\n",
            "V0221 12:19:58.466000 9693 torch/_dynamo/guards.py:2789] [5/3] [__recompiles_verbose]     - 5/1: tensor 'L['self']._modules['lora_B']._modules['default']._parameters['weight']' size mismatch at index 0. expected 512, actual 2048. Guard failed on a parameter, consider using torch._dynamo.config.force_parameter_static_shapes = False to allow dynamism on parameters.\n",
            "W0221 12:19:58.801000 9693 torch/_inductor/debug.py:435] [5/3] model__10_forward_24 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__10_forward_24.16\n",
            "W0221 12:19:58.845000 9693 torch/_inductor/debug.py:435] [5/3] model__10_backward_25 debug trace: /content/torch_compile_debug/run_2025_02_21_12_19_39_187755-pid_9693/torchinductor/model__10_backward_25.17\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:05, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.518900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.393400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.502700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.534200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.138600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.979400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.249100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.630700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.222700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.686300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=2.385595905780792, metrics={'train_runtime': 27.3406, 'train_samples_per_second': 0.732, 'train_steps_per_second': 0.366, 'total_flos': 10592155496448.0, 'train_loss': 2.385595905780792})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}