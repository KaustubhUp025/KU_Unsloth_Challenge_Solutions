{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2086e5-mHPP8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install unsloth\n",
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldrmp2gQHUXp",
        "outputId": "79afb94a-8f3c-4c46-b84c-c189eee32472"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Collecting triton\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires triton==3.1.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\", but you have triton 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed triton-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ],
      "metadata": {
        "id": "MwEyvr16HXZU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables used in fast_dequantize.\n",
        "CUDA_STREAM = None\n",
        "WEIGHT_BUFFER = None\n",
        "ABSMAX_BUFFER = None\n",
        "\n",
        "# ===== Step 1: Monkey-patch dataclasses.fields (if needed) =====\n",
        "import dataclasses\n",
        "\n",
        "# Save the original dataclasses.fields.\n",
        "_original_fields = dataclasses.fields\n",
        "\n",
        "def safe_fields(obj):\n",
        "    try:\n",
        "        return _original_fields(obj)\n",
        "    except TypeError:\n",
        "        # If obj is not a dataclass, return an empty tuple (or customize as needed)\n",
        "        return tuple()\n",
        "\n",
        "# Monkey-patch fields to avoid errors during unsloth imports.\n",
        "dataclasses.fields = safe_fields\n",
        "\n",
        "# ===== Step 2: Import required modules =====\n",
        "import torch\n",
        "from bitsandbytes.nn import Linear4bit\n",
        "from transformers import set_seed\n",
        "from transformers.activations import ACT2FN\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "\n",
        "# Now import unsloth utilities (which might call dataclasses.fields).\n",
        "from unsloth.kernels.utils import (\n",
        "    get_ptr,\n",
        "    cdequantize_blockwise_fp32,\n",
        "    ctypes_c_int,\n",
        "    cdequantize_blockwise_fp16_nf4,\n",
        "    cdequantize_blockwise_bf16_nf4\n",
        ")\n",
        "\n",
        "# ===== Step 3: Import dataclasses utilities and define our override =====\n",
        "from dataclasses import is_dataclass, dataclass\n",
        "\n",
        "# Define a new dataclass for the quantization state.\n",
        "@dataclass\n",
        "class NewQuantState:\n",
        "    absmax: torch.Tensor\n",
        "    code: torch.Tensor\n",
        "    offset: torch.Tensor   # or float, depending on your usage\n",
        "    blocksize: int\n",
        "    dtype: torch.dtype\n",
        "    state2: object       # refine this type if possible\n",
        "    shape: tuple         # shape of the dequantized weight matrix\n",
        "\n",
        "# Helper: ensure the quant_state is a dataclass instance.\n",
        "def ensure_dataclass_quant_state(qs):\n",
        "    if is_dataclass(qs):\n",
        "        return qs\n",
        "    return NewQuantState(\n",
        "        absmax = qs.absmax,\n",
        "        code = qs.code,\n",
        "        offset = qs.offset,\n",
        "        blocksize = qs.blocksize,\n",
        "        dtype = qs.dtype,\n",
        "        state2 = qs.state2,\n",
        "        shape = qs.shape\n",
        "    )\n",
        "\n",
        "# New fast_dequantize implementation using our helper.\n",
        "def new_fast_dequantize(W, quant_state=None, out=None, use_global_buffer=False):\n",
        "    if quant_state is None:\n",
        "        return W\n",
        "    if type(quant_state) is not list:\n",
        "        quant_state = ensure_dataclass_quant_state(quant_state)\n",
        "        absmax     = quant_state.absmax\n",
        "        shape      = quant_state.shape\n",
        "        dtype      = quant_state.dtype\n",
        "        blocksize  = quant_state.blocksize\n",
        "        offset     = quant_state.offset\n",
        "        state2     = quant_state.state2\n",
        "        absmax2    = state2.absmax\n",
        "        code2      = state2.code\n",
        "        blocksize2 = state2.blocksize\n",
        "    else:\n",
        "        # Handle the old quant_state (if needed)\n",
        "        absmax, shape, dtype, blocksize, compressed_stats, _, _ = quant_state\n",
        "        offset, state2 = compressed_stats\n",
        "        absmax2, code2, blocksize2, _, _, _, _ = state2\n",
        "    global CUDA_STREAM\n",
        "    if CUDA_STREAM is None:\n",
        "        CUDA_STREAM = torch.cuda.current_stream(\"cuda:0\")\n",
        "    n_elements_absmax = absmax.numel()\n",
        "    if use_global_buffer:\n",
        "        size = shape[0] * shape[1]\n",
        "        global WEIGHT_BUFFER, ABSMAX_BUFFER\n",
        "        if WEIGHT_BUFFER is None:\n",
        "            WEIGHT_BUFFER = torch.empty(size, dtype=dtype, device=\"cuda:0\", requires_grad=False)\n",
        "            ABSMAX_BUFFER = torch.empty(n_elements_absmax, dtype=torch.float32, device=\"cuda:0\", requires_grad=False)\n",
        "        if size > WEIGHT_BUFFER.numel():\n",
        "            WEIGHT_BUFFER.resize_(size)\n",
        "        if n_elements_absmax > ABSMAX_BUFFER.numel():\n",
        "            ABSMAX_BUFFER.resize_(n_elements_absmax)\n",
        "        out = WEIGHT_BUFFER[:size].view(shape)\n",
        "        out_absmax = ABSMAX_BUFFER[:n_elements_absmax]\n",
        "    else:\n",
        "        if out is None:\n",
        "            out = torch.empty(shape, dtype=dtype, device=\"cuda:0\", requires_grad=False)\n",
        "        else:\n",
        "            assert(out.shape == shape)\n",
        "            assert(out.dtype == dtype)\n",
        "        out_absmax = torch.empty(n_elements_absmax, dtype=torch.float32, device=\"cuda:0\", requires_grad=False)\n",
        "    ptr_out_absmax = get_ptr(out_absmax)\n",
        "    cdequantize_blockwise_fp32(\n",
        "        get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), ptr_out_absmax,\n",
        "        ctypes_c_int(blocksize2), ctypes_c_int(n_elements_absmax), CUDA_STREAM,\n",
        "    )\n",
        "    out_absmax += offset\n",
        "    fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else cdequantize_blockwise_bf16_nf4\n",
        "    fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),\n",
        "       ctypes_c_int(blocksize), ctypes_c_int(out.numel()), CUDA_STREAM)\n",
        "    is_transposed = (True if W.shape[0] == 1 else False)\n",
        "    return out.t() if is_transposed else out\n",
        "\n",
        "# Override the fast_dequantize in unsloth.kernels.utils.\n",
        "import unsloth.kernels.utils as uutils\n",
        "uutils.fast_dequantize = new_fast_dequantize\n",
        "\n",
        "# (Optionally restore dataclasses.fields here if needed)\n",
        "dataclasses.fields = _original_fields\n",
        "\n",
        "# ===== Step 4: Define your model and test functions =====\n",
        "def dequantize_wrapper(module):\n",
        "    # module is a Linear4bit instance; extract its weight and quant_state.\n",
        "    return new_fast_dequantize(module.weight, module.weight.quant_state)\n",
        "\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype=torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias=None,\n",
        "        compute_dtype=dtype,\n",
        "        compress_statistics=True,\n",
        "        quant_type=\"nf4\",\n",
        "    )\n",
        "\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.dtype == dtype)\n",
        "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.blocksize == 64)\n",
        "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
        "\n",
        "import torch.nn as nn\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd=4096, m=14336, dtype=torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype=dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype=dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype=dtype).to(\"cuda\")\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj.weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up = X @ fx(mlp.up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    import time\n",
        "    elapsed = 0\n",
        "    options = [\n",
        "        (2, 3333, 2048, 8192, 3407, torch.float16),\n",
        "        (5, 777, 1024, 4096, 3409, torch.bfloat16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd=hd, m=m, dtype=dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device=\"cuda\", dtype=dt)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup: use dequantize_wrapper everywhere.\n",
        "        for _ in range(2):\n",
        "            # Use the wrapper so that a tensor is returned.\n",
        "            assert_same(mlp_forward(X, mlp, dequantize_wrapper), mlp(X), _F(_C()), dt)\n",
        "            assert_correct_bnb(mlp.up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_wrapper)\n",
        "            A, B, C = mlp_dequantize(X, mlp, dequantize_wrapper)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000):\n",
        "            mlp_dequantize(X, mlp, dequantize_wrapper)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed\n",
        "\n",
        "# ===== Step 5: Run your tests =====\n",
        "elapsed = test_dequantize(dequantize_wrapper)\n",
        "print(\"Elapsed time for your kernel: \", elapsed)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8JSRnYQHZTq",
        "outputId": "b09cd21e-b2d8-483b-c7b5-8ed6e22cf473"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "Elapsed time for your kernel:  5.022521495819092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from triton import jit, cdiv\n",
        "import triton.language as tl\n",
        "\n",
        "# Define a tile size: each thread will process TILE outputs.\n",
        "TILE = 16\n",
        "\n",
        "@jit\n",
        "def _your_dequantize_nf4_kernel_tiled(\n",
        "    weight_ptr,             # pointer to the uint8 weight tensor (each byte packs 2 nf4 values)\n",
        "    absmax_ptr,             # pointer to absmax tensor (dtype=torch.uint8)\n",
        "    nested_absmax_ptr,      # pointer to nested_absmax tensor (dtype=torch.float32)\n",
        "    out_ptr,                # pointer to output tensor (fp16 or bf16)\n",
        "    offset,                 # scalar offset (float)\n",
        "    num_elements: tl.constexpr,   # total number of dequantized outputs\n",
        "    blocksize: tl.constexpr,      # block size for weight dequantization (e.g., 64)\n",
        "    nested_ratio: tl.constexpr,   # number of weight blocks per nested scaling factor (e.g., 256/64 = 4)\n",
        "    BLOCK_SIZE: tl.constexpr      # total outputs processed per kernel instance (must be a multiple of TILE)\n",
        "):\n",
        "    # Each kernel instance processes BLOCK_SIZE outputs.\n",
        "    # Each thread processes TILE outputs.\n",
        "    THREADS = BLOCK_SIZE // TILE\n",
        "\n",
        "    # Get thread index (per kernel instance) as int32.\n",
        "    tid = tl.arange(0, THREADS)\n",
        "    tid = tl.cast(tid, tl.int32)\n",
        "    # Compute a tile of global indices for each thread.\n",
        "    base = tl.cast(tid, tl.int32) * TILE  # starting index for each thread within the kernel instance\n",
        "    # Global base index for this kernel instance.\n",
        "    pid = tl.cast(tl.program_id(0), tl.int32)\n",
        "    global_base = pid * BLOCK_SIZE\n",
        "    # Each thread will compute indices: global_base + base + i, for i in [0, TILE)\n",
        "    # We'll loop over i.\n",
        "    for i in range(TILE):\n",
        "        idx = global_base + base + i  # each idx is int32, shape (THREADS,)\n",
        "        m = idx < tl.cast(num_elements, tl.int32)  # mask\n",
        "\n",
        "        # Compute weight block index: each block covers 'blocksize' outputs.\n",
        "        bs = int(blocksize)   # e.g., 64\n",
        "        nr = int(nested_ratio)  # e.g., 4\n",
        "        wb = tl.floordiv(idx, bs)       # weight block index (int32)\n",
        "        nest_idx = tl.floordiv(wb, nr)    # nested index (int32)\n",
        "\n",
        "        # Asynchronously copy scaling factors from global memory.\n",
        "        # (If tl.copy_async is not available, this call will fallback to tl.load.)\n",
        "        abs_val = tl.cast(tl.copy_async(absmax_ptr + wb), tl.float32)\n",
        "        nest_val = tl.cast(tl.copy_async(nested_absmax_ptr + nest_idx), tl.float32)\n",
        "        tl.wait_async()  # Ensure async copy has completed.\n",
        "        scale = (abs_val / 255.0) * (nest_val / 255.0) + offset\n",
        "\n",
        "        # For the weight: each uint8 packs 2 nf4 values.\n",
        "        # Compute the byte index: each output corresponds to an nf4 value.\n",
        "        byte_index = tl.floordiv(idx, 2)\n",
        "        byte_index = tl.cast(byte_index, tl.int32)\n",
        "        weight_byte = tl.load(weight_ptr + byte_index, mask=m)\n",
        "        # Determine if this output comes from lower or upper 4 bits.\n",
        "        is_even = (idx % 2) == 0\n",
        "        lower_val = weight_byte & 0x0F\n",
        "        upper_val = (weight_byte >> 4) & 0x0F\n",
        "        quant_val = tl.where(is_even, lower_val, upper_val)\n",
        "        # Dequantize: normalize to [0,1] and multiply by scale.\n",
        "        dequant = (tl.cast(quant_val, tl.float32) / 15.0) * scale\n",
        "\n",
        "        tl.store(out_ptr + idx, dequant, mask=m)\n",
        "\n",
        "###############################################\n",
        "# Launcher and Wrapper Functions\n",
        "###############################################\n",
        "def _your_dequantize_nf4(weight, quant_state):\n",
        "    shape = quant_state.shape\n",
        "    total_elements = shape[0] * shape[1]\n",
        "    out = torch.empty(total_elements, dtype=quant_state.dtype, device=weight.device)\n",
        "\n",
        "    blocksize = quant_state.blocksize         # e.g., 64\n",
        "    nested_ratio = quant_state.state2.blocksize // blocksize  # e.g., 256 // 64 = 4\n",
        "    offset = float(quant_state.offset.item()) if isinstance(quant_state.offset, torch.Tensor) else float(quant_state.offset)\n",
        "\n",
        "    weight_ptr = weight.data_ptr()\n",
        "    absmax_ptr = quant_state.absmax.data_ptr()\n",
        "    nested_absmax_ptr = quant_state.state2.absmax.data_ptr()\n",
        "    out_ptr = out.data_ptr()\n",
        "\n",
        "    # Choose a BLOCK_SIZE that is a multiple of TILE.\n",
        "    BLOCK_SIZE = 1024\n",
        "    grid = lambda meta: (cdiv(total_elements, meta['BLOCK_SIZE']),)\n",
        "\n",
        "    _your_dequantize_nf4_kernel_tiled[grid](\n",
        "        weight_ptr,\n",
        "        absmax_ptr,\n",
        "        nested_absmax_ptr,\n",
        "        out_ptr,\n",
        "        offset,\n",
        "        total_elements,\n",
        "        blocksize,\n",
        "        nested_ratio,\n",
        "        BLOCK_SIZE,\n",
        "    )\n",
        "\n",
        "    out = out.view(shape)\n",
        "    if weight.shape[0] == 1:\n",
        "        out = out.t()\n",
        "    return out\n",
        "\n",
        "def your_dequantize_nf4(weight_obj):\n",
        "    \"\"\"\n",
        "    weight_obj: an object with attributes:\n",
        "         - weight_obj.weight.data         (torch.Tensor, dtype=torch.uint8)\n",
        "         - weight_obj.weight.quant_state  (with the quantization state as described)\n",
        "    \"\"\"\n",
        "    return _your_dequantize_nf4(weight_obj.weight.data, weight_obj.weight.quant_state)\n",
        "\n",
        "\n",
        "\n",
        "elapsed = test_dequantize(your_dequantize_nf4)\n",
        "print(\"Elapsed time for your kernel: \", elapsed)\n",
        "\n",
        "print(test_dequantize(dequantize_wrapper) / test_dequantize(your_dequantize_nf4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64Y0mNCLHdjY",
        "outputId": "2898980a-b198-4af4-d5a1-46c504bb35f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time for your kernel:  5.098713159561157\n",
            "1.0129438812848506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JsgLlqm2HgDX"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}